import torch
import lightning.pytorch as pl
import hydra
from monai.inferers import SlidingWindowInferer
from src.utils.metrics import Evaluator
import torch.nn.functional as F # å¼•å…¥ F ç”¨äºå¯èƒ½çš„å¯¹é½

class VesselSystem(pl.LightningModule):
    def __init__(self, model, cfg):
        super().__init__()
        self.save_hyperparameters(ignore=['model'])
        self.model = model
        self.cfg = cfg
        
        # å®ä¾‹åŒ– Loss
        self.main_criterion = hydra.utils.instantiate(cfg.trainer.lightning_module.loss)
        self.aux_criterion = torch.nn.BCEWithLogitsLoss()
        self.evaluator = Evaluator()

        # âœ… å®Œç¾é‡ç°åŸç‰ˆå®ç°ï¼šç›´æ¥ä»é…ç½®è¯»å–æ»‘åŠ¨çª—å£å‚æ•°
        lm_cfg = cfg.trainer.lightning_module
        self.inferer = SlidingWindowInferer(
            roi_size=lm_cfg.roi_size,          # [96, 96, 96]
            sw_batch_size=lm_cfg.sw_batch_size, # 4
            overlap=lm_cfg.overlap,            # 0.5
            mode=lm_cfg.mode                   # "gaussian"
        )

        self.prediction_threshold = lm_cfg.prediction_threshold

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        img, mask, _ = batch
        pred_mask, aux_dict = self.model(img)
        
        # âœ… å®Œç¾å¯¹é½ï¼šä½¿ç”¨ PyTorch åŸç”Ÿåˆ‡ç‰‡å®ç°ä¸­å¿ƒè£å‰ª
        # å½»åº•è§£å†³ "ImportError: cannot import name 'center_spatial_crop'"
        if pred_mask.shape != mask.shape:
            # è·å–ä¸¤è€…çš„ç»´åº¦å·®å¼‚
            # å‡è®¾å½¢çŠ¶ä¸º [B, C, D, H, W]
            diff_d = (pred_mask.shape[2] - mask.shape[2]) // 2
            diff_h = (pred_mask.shape[3] - mask.shape[3]) // 2
            diff_w = (pred_mask.shape[4] - mask.shape[4]) // 2
            
            # æ‰§è¡Œåˆ‡ç‰‡è£å‰ª
            pred_mask = pred_mask[:, :, 
                                  diff_d : diff_d + mask.shape[2],
                                  diff_h : diff_h + mask.shape[3],
                                  diff_w : diff_w + mask.shape[4]]
        
        # è®¡ç®—ä¸»åˆ†å‰²æŸå¤± (Dice + CE)
        loss_seg = self.main_criterion(pred_mask, mask.float())

        # 2. ğŸš€ æ–°å¢ï¼šæŠ•å½±ä¸€è‡´æ€§æŸå¤± (Consistency Loss)
        loss_consist = 0
        if "pred_3d_mip" in aux_dict:
            # ç”Ÿæˆé‡‘æ ‡(Mask)çš„åˆ†å±‚ MIP æŠ•å½±ä½œä¸ºç›®æ ‡
            with torch.no_grad():
                mask_mip = self.model._slab_mip(mask.float(), self.model.slab_thickness)
            
            # å¼ºåˆ¶ 3D é¢„æµ‹çš„æŠ•å½±å›¾(Sigmoidå)ä¸é‡‘æ ‡æŠ•å½±å›¾ä¸€è‡´
            # è¿™é‡Œä½¿ç”¨ MSE æŸå¤±æ¥çº¦æŸæ¦‚ç‡åˆ†å¸ƒ
            loss_consist = F.mse_loss(aux_dict["pred_3d_mip"].sigmoid(), mask_mip)
        
        # è®¡ç®—æ‹“æ‰‘æ„ŸçŸ¥è¾…åŠ©æŸå¤±
        loss_topo = 0
        topo_count = 0
        for aux in aux_dict.values():
            if isinstance(aux, dict) and "P_pred" in aux and "P_pseudo" in aux:
                loss_topo += self.aux_criterion(aux["P_pred"], aux["P_pseudo"])
                topo_count += 1
        
        # ç»„åˆæ€»æŸå¤±
        lambda_topo = getattr(self.cfg.model, "lambda_topo", 0.3)
        #total_loss = loss_seg + (lambda_topo * loss_topo if topo_count > 0 else 0)
        lambda_consist = 0.2  # ğŸš€ è¿™æ˜¯æ–°å¢çš„ä¸€è‡´æ€§æŸå¤±æƒé‡ï¼Œå¯è°ƒ
        
        total_loss = loss_seg + \
                     (lambda_topo * loss_topo if topo_count > 0 else 0) + \
                     (lambda_consist * loss_consist if loss_consist != 0 else 0)
        
        # è®°å½•æ—¥å¿—
        self.log("train/loss", total_loss, prog_bar=True)

        self.log("train/loss_consist", loss_consist, prog_bar=False)
        return total_loss

    def validation_step(self, batch, batch_idx):
        img, mask, name = batch
        
        # æ»‘åŠ¨çª—å£æ¨ç†
        pred_mask = self.inferer(img, lambda x: self.model(x)[0])
        
        # è®¡ç®— Loss
        loss = self.main_criterion(pred_mask, mask.float())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.evaluator.calculate_all(pred_mask.sigmoid(), mask)
        
        # âœ… ä¿®æ”¹ï¼šæ·»åŠ  prog_bar=True å³å¯åœ¨ç»ˆç«¯è¿›åº¦æ¡å®æ—¶çœ‹åˆ°æ•°å€¼
        self.log("val/loss", loss, prog_bar=True)
        self.log("val/dice", metrics["Dice"], prog_bar=True)
        self.log("val/cldice", metrics["clDice"], prog_bar=True)
        
        return loss

    def configure_optimizers(self):
        # 1. å®ä¾‹åŒ–ä¼˜åŒ–å™¨é…ç½®
        opt_cfg = self.cfg.trainer.lightning_module.optimizer_factory
        optimizer_obj = hydra.utils.instantiate(opt_cfg, params=self.parameters())
        
        # âœ… æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœè¿”å›çš„æ˜¯ partial å¯¹è±¡ï¼Œæ‰§è¡Œå®ƒä»¥è·å¾—çœŸå®çš„ä¼˜åŒ–å™¨
        if isinstance(optimizer_obj, torch.optim.Optimizer):
            optimizer = optimizer_obj
        else:
            # è¿™æ˜¯ä¸€ä¸ª partial å¯¹è±¡ï¼Œæ‰‹åŠ¨è°ƒç”¨å®ƒ
            optimizer = optimizer_obj() 
        
        print(f"[DEBUG] Real Optimizer: {type(optimizer).__name__}")

        # 2. å¤„ç†è°ƒåº¦å™¨
        if "scheduler_configs" in self.cfg.trainer.lightning_module:
            lr_schedulers = []
            sched_configs = self.cfg.trainer.lightning_module.scheduler_configs
            
            for name, s_cfg in sched_configs.items():
                scheduler_obj = hydra.utils.instantiate(s_cfg.scheduler, optimizer=optimizer)
                
                # âœ… åŒæ ·çš„ä¿®å¤ï¼šå¤„ç†å¯èƒ½å­˜åœ¨çš„ partial è°ƒåº¦å™¨
                if hasattr(scheduler_obj, "__call__") and not isinstance(scheduler_obj, torch.optim.lr_scheduler.LRScheduler):
                    scheduler = scheduler_obj()
                else:
                    scheduler = scheduler_obj

                lr_schedulers.append({
                    "scheduler": scheduler,
                    "interval": s_cfg.interval,
                    "frequency": s_cfg.frequency,
                    "name": f"lr/{name}"
                })
                print(f"[DEBUG] Real Scheduler added: {name}")

            return {
                "optimizer": optimizer,
                "lr_scheduler": lr_schedulers
            }
            
        return optimizer